{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data folder relative to the notebook\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")\n",
    "ace_violations = pd.read_csv(os.path.join(DATA_DIR, \"ACE_violations.csv\"))\n",
    "#ace_violations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "features = []\n",
    "\n",
    "for _, row in ace_violations.iterrows():\n",
    "    try:\n",
    "        v_lat, v_lon = row[\"Violation Latitude\"], row[\"Violation Longitude\"]\n",
    "\n",
    "        # Skip rows with bad coords\n",
    "        if (\n",
    "            pd.isna(v_lat) or pd.isna(v_lon) or\n",
    "            not (math.isfinite(v_lat) and math.isfinite(v_lon))\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        # Properties (convert everything to string to avoid JSON issues)\n",
    "        props = {col: str(row[col]) for col in [\n",
    "            \"Violation ID\",\n",
    "            \"Vehicle ID\",\n",
    "            \"First Occurrence\",\n",
    "            \"Last Occurrence\",\n",
    "            \"Violation Status\",\n",
    "            \"Violation Type\",\n",
    "            \"Bus Route ID\",\n",
    "            \"Stop ID\",\n",
    "            \"Stop Name\"\n",
    "        ] if col in row}\n",
    "\n",
    "        feature = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": props,\n",
    "            \"geometry\": {\n",
    "                \"type\": \"Point\",\n",
    "                \"coordinates\": [float(v_lon), float(v_lat)]\n",
    "            }\n",
    "        }\n",
    "        features.append(feature)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping row due to error: {e}\")\n",
    "\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": features\n",
    "}\n",
    "\n",
    "with open(\"/Users/danielbrown/Desktop/datathon_project/data/processed/violations.geojson\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(geojson, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… Saved {len(features)} point features to violations_points.geojson\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview first 3 features\n",
    "preview = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": features[:3]\n",
    "}\n",
    "\n",
    "#print(json.dumps(preview, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bus Speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data/raw/bus_speed_2020_2024.csv\n",
    "#data/raw/bus_speed_2025.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data folder relative to the notebook\n",
    "DATA_DIR = os.path.join(\"..\", \"data\", \"raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_speed_2020_2024 = pd.read_csv(os.path.join(DATA_DIR, \"segment_speed_2023_2024.csv\"))\n",
    "segment_speed_2025 = pd.read_csv(os.path.join(DATA_DIR, \"segment_speed_2025.csv\"))\n",
    "# Concatenate vertically (stack rows)\n",
    "segment_speed_all = pd.concat([segment_speed_2020_2024, segment_speed_2025], ignore_index=True)\n",
    "\n",
    "# Optional: check the combined shape\n",
    "#print(segment_speed_all.shape)\n",
    "#print(segment_speed_all.head())\n",
    "df = segment_speed_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoJSON created with 173 features\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# --- 1. Load GTFS shapes and trips ---\n",
    "shapes = pd.read_csv(\"/Users/danielbrown/Desktop/gtfs_bx-3/shapes.txt\")\n",
    "shapes = shapes.sort_values([\"shape_id\", \"shape_pt_sequence\"])\n",
    "\n",
    "trips = pd.read_csv(\"/Users/danielbrown/Desktop/gtfs_bx-3/trips.txt\")\n",
    "\n",
    "# --- 2. Load speeds ---\n",
    "speeds = pd.read_csv(\n",
    "    \"/Users/danielbrown/Desktop/MTA_Bus_Route_Segment_Speeds__Beginning_2025_20250920.csv\",\n",
    "    parse_dates=[\"Timestamp\"]\n",
    ")\n",
    "\n",
    "# --- 2b. Aggregate speeds by Next Timepoint Stop Name and Direction ---\n",
    "agg_speeds = (\n",
    "    speeds.groupby([\"Next Timepoint Stop Name\", \"Direction\"], as_index=False)\n",
    "    .agg(\n",
    "        avg_speed=(\"Average Road Speed\", \"mean\"),\n",
    "        total_trips=(\"Bus Trip Count\", \"sum\"),\n",
    "        start_lat=(\"Timepoint Stop Latitude\", \"first\"),\n",
    "        start_lon=(\"Timepoint Stop Longitude\", \"first\"),\n",
    "        end_lat=(\"Next Timepoint Stop Latitude\", \"first\"),\n",
    "        end_lon=(\"Next Timepoint Stop Longitude\", \"first\"),\n",
    "        route_id=(\"Route ID\", \"first\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 2c. Map directions and add segment numbers ---\n",
    "dir_map = {'W': 0, 'E': 1}  # adjust if needed\n",
    "agg_speeds['direction_id'] = agg_speeds['Direction'].map(dir_map)\n",
    "\n",
    "# Number segments within each route + direction\n",
    "agg_speeds = agg_speeds.sort_values([\"route_id\", \"direction_id\"])\n",
    "agg_speeds[\"segment_number\"] = (\n",
    "    agg_speeds.groupby([\"route_id\", \"direction_id\"]).cumcount() + 1\n",
    ")\n",
    "\n",
    "# --- 3. Helper functions ---\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371e3\n",
    "    phi1, phi2 = radians(lat1), radians(lat2)\n",
    "    dphi, dlambda = radians(lat2 - lat1), radians(lon2 - lon1)\n",
    "    a = sin(dphi/2)**2 + cos(phi1) * cos(phi2) * sin(dlambda/2)**2\n",
    "    return 2 * R * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "def closest_index(lat, lon, coords):\n",
    "    return min(range(len(coords)), key=lambda i: haversine(lat, lon, coords[i][1], coords[i][0]))\n",
    "\n",
    "# --- 4. Map route+direction to shape_id ---\n",
    "direction_map = trips.groupby(['route_id', 'direction_id'])['shape_id'].first().to_dict()\n",
    "\n",
    "# --- 5. Build GeoJSON ---\n",
    "features = []\n",
    "\n",
    "for _, row in agg_speeds.iterrows():\n",
    "    route_dir_key = (row[\"route_id\"], row[\"direction_id\"])\n",
    "    if route_dir_key not in direction_map:\n",
    "        continue\n",
    "    shape_id = direction_map[route_dir_key]\n",
    "    route_shape = shapes[shapes[\"shape_id\"] == shape_id]\n",
    "    shape_coords = list(zip(route_shape[\"shape_pt_lon\"], route_shape[\"shape_pt_lat\"]))\n",
    "\n",
    "    # Find indices along shape for stop and next stop\n",
    "    i1 = closest_index(row[\"start_lat\"], row[\"start_lon\"], shape_coords)\n",
    "    i2 = closest_index(row[\"end_lat\"], row[\"end_lon\"], shape_coords)\n",
    "    if i1 > i2:\n",
    "        i1, i2 = i2, i1\n",
    "\n",
    "    # Fallback if points are the same\n",
    "    if i1 == i2:\n",
    "        segment_coords = [(row[\"start_lon\"], row[\"start_lat\"]), (row[\"end_lon\"], row[\"end_lat\"])]\n",
    "    else:\n",
    "        segment_coords = shape_coords[i1:i2+1]\n",
    "\n",
    "    features.append({\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\"type\": \"LineString\", \"coordinates\": segment_coords},\n",
    "        \"properties\": {\n",
    "            \"route_id\": row[\"route_id\"],\n",
    "            \"direction\": row[\"Direction\"],\n",
    "            \"segment_number\": int(row[\"segment_number\"]),\n",
    "            \"speed\": row[\"avg_speed\"],\n",
    "            \"trips\": row[\"total_trips\"]\n",
    "        }\n",
    "    })\n",
    "\n",
    "geojson = {\"type\": \"FeatureCollection\", \"features\": features}\n",
    "\n",
    "with open(\"/Users/danielbrown/Desktop/bx12_speeds_shapes_directions_agg_1.geojson\", \"w\") as f:\n",
    "    json.dump(geojson, f)\n",
    "\n",
    "print(f\"GeoJSON created with {len(features)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing communities around bus routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Income Columns:\n",
      " Index(['Location', 'All Households', 'Families', 'Families with Children',\n",
      "       'Families without Children'],\n",
      "      dtype='object') \n",
      "\n",
      "CDTA Columns:\n",
      " Index(['BoroCode', 'BoroName', 'CountyFIPS', 'CDTA2020', 'CDTAName',\n",
      "       'CDTAType', 'Shape_Length', 'Shape_Area', 'the_geom'],\n",
      "      dtype='object') \n",
      "\n",
      "Merged DataFrame size: 71 rows x 9 columns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "median_income_fp = '/Users/danielbrown/Desktop/Median_Incomes - Sheet1.csv'\n",
    "cdta_fp = '/Users/danielbrown/Desktop/2020_Community_District_Tabulation_Areas__CDTAs__20250920.csv'\n",
    "\n",
    "# Load CSVs\n",
    "median_income_df = pd.read_csv(median_income_fp)\n",
    "cdta_df = pd.read_csv(cdta_fp)\n",
    "\n",
    "# Quick check\n",
    "# Check columns\n",
    "print(\"Median Income Columns:\\n\", median_income_df.columns, \"\\n\")\n",
    "print(\"CDTA Columns:\\n\", cdta_df.columns, \"\\n\")\n",
    "\n",
    "# Get the size of the dataframe\n",
    "rows, cols = cdta_df.shape\n",
    "print(f\"Merged DataFrame size: {rows} rows x {cols} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Location All Households  Families Families with Children  \\\n",
      "0               Astoria(Q01)        $84,590   $94,918                $85,568   \n",
      "1  Battery Park/Tribeca(M01)       $198,945  $250,000               $250,000   \n",
      "2             Bay Ridge(K10)        $88,566  $100,176               $108,244   \n",
      "3               Bayside(Q11)       $107,607  $126,636               $124,228   \n",
      "4          Bedford Park(B07)        $42,387   $47,368                $35,920   \n",
      "\n",
      "  Families without Children  Code  \n",
      "0                  $110,222  QN01  \n",
      "1                  $250,000  MN01  \n",
      "2                   $97,493  BK10  \n",
      "3                  $127,902  QN11  \n",
      "4                   $55,125  BX07  \n",
      "Merged DataFrame size: 59 rows x 6 columns\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract the code in parentheses\n",
    "median_income_df['Code'] = median_income_df['Location'].str.extract(r'\\((.*?)\\)')\n",
    "\n",
    "# Step 2: Replace letters only (prefix replacements)\n",
    "letter_map = {\n",
    "    \"K\": \"BK\",\n",
    "    \"S\": \"SI\",\n",
    "    \"Q\": \"QN\",\n",
    "    \"B\": \"BX\",\n",
    "    \"M\": \"MN\"\n",
    "}\n",
    "\n",
    "# Function to replace letters while keeping numbers\n",
    "def replace_prefix(code):\n",
    "    if pd.isna(code):\n",
    "        return code\n",
    "    for old, new in letter_map.items():\n",
    "        if code.startswith(old):\n",
    "            return new + code[len(old):]\n",
    "    return code  # leave as-is if no match\n",
    "\n",
    "median_income_df['Code'] = median_income_df['Code'].apply(replace_prefix)\n",
    "\n",
    "# Optional: check first few rows\n",
    "print(median_income_df.head())\n",
    "\n",
    "# Get the size of the dataframe\n",
    "rows, cols = median_income_df.shape\n",
    "print(f\"Merged DataFrame size: {rows} rows x {cols} columns\")\n",
    "\n",
    "# Step 3: Export to CSV\n",
    "#output_fp = '/Users/danielbrown/Desktop/median_income_with_codes.csv'\n",
    "#median_income_df.to_csv(output_fp, index=False)\n",
    "#print(f\"Saved updated CSV to {output_fp}\")\n",
    "#print(\"Median Income Columns:\\n\", median_income_df.columns, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Location  All Households  Families  \\\n",
      "0   Williamsburg/Greenpoint(K01)        111492.0  101739.0   \n",
      "1  Fort Greene/Brooklyn Hts(K02)        151134.0  208098.0   \n",
      "2        Bedford Stuyvesant(K03)         75184.0   76551.0   \n",
      "3                  Bushwick(K04)         78456.0   78290.0   \n",
      "4             East New York(K05)         49345.0   63142.0   \n",
      "\n",
      "   Families with Children  Families without Children  Code  \\\n",
      "0                152567.0                    94261.0  BK01   \n",
      "1                242917.0                   196421.0  BK02   \n",
      "2                 61606.0                    82823.0  BK03   \n",
      "3                 52319.0                    82353.0  BK04   \n",
      "4                 56703.0                    70556.0  BK05   \n",
      "\n",
      "                                            the_geom  \n",
      "0  MULTIPOLYGON (((-73.92405909736993 40.71411156...  \n",
      "1  MULTIPOLYGON (((-73.96929296348496 40.70709333...  \n",
      "2  MULTIPOLYGON (((-73.91804606892461 40.68721324...  \n",
      "3  MULTIPOLYGON (((-73.89652644236399 40.68240329...  \n",
      "4  MULTIPOLYGON (((-73.88828531356056 40.64672241...  \n",
      "Index(['Location', 'All Households', 'Families', 'Families with Children',\n",
      "       'Families without Children', 'Code', 'the_geom'],\n",
      "      dtype='object')\n",
      "Merged DataFrame size: 59 rows x 7 columns\n",
      "Saved merged CSV to /Users/danielbrown/Desktop/median_income_cdta_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:17: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\$'\n",
      "/var/folders/r2/62xph3kj5rj_w51slp5lpvyc0000gn/T/ipykernel_63828/38112018.py:17: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  merged_df[col] = merged_df[col].replace('[\\$,]', '', regex=True).astype(float)\n"
     ]
    }
   ],
   "source": [
    "# Inner join on Code -> CDTA2020\n",
    "merged_df = median_income_df.merge(\n",
    "    cdta_df,\n",
    "    left_on='Code',\n",
    "    right_on='CDTA2020',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Sort the DataFrame by 'Code'\n",
    "merged_df = merged_df.sort_values(by='Code').reset_index(drop=True)\n",
    "\n",
    "merged_df = merged_df[['Location','All Households','Families','Families with Children','Families without Children','Code','the_geom']]\n",
    "\n",
    "# Step 1: Strip $ and commas from income columns and convert to numeric\n",
    "income_cols = ['All Households', 'Families', 'Families with Children', 'Families without Children']\n",
    "for col in income_cols:\n",
    "    merged_df[col] = merged_df[col].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# Step 2: Ensure string columns\n",
    "string_cols = ['Location', 'Code', 'the_geom']\n",
    "for col in string_cols:\n",
    "    merged_df[col] = merged_df[col].astype(str)\n",
    "\n",
    "# Quick check\n",
    "print(merged_df.head())\n",
    "print(merged_df.columns)\n",
    "# Get the size of the dataframe\n",
    "rows, cols = merged_df.shape\n",
    "print(f\"Merged DataFrame size: {rows} rows x {cols} columns\")\n",
    "\n",
    "# Optional: export to CSV\n",
    "output_fp = '/Users/danielbrown/Desktop/median_income_cdta_merged.csv'\n",
    "merged_df.to_csv(output_fp, index=False)\n",
    "print(f\"Saved merged CSV to {output_fp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (datathon_env)",
   "language": "python",
   "name": "datathon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
